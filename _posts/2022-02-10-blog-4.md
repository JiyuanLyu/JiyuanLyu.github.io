---
layout: post
title: Blog Post 4
---

In this blog post, I'll write a simple tutorial of `Spectral clustering` for clustering data points. 

### Notation

In all the math below: 

- Boldface capital letters like $\mathbf{A}$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $\mathbf{v}$ refer to vectors (1d arrays of numbers). 
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 

## Introduction

*Spectral clustering* is an important tool for identifying meaningful parts of data sets with complex structure. In this part, we'll study some example where we *don't* need spectral clustering. 


```python
# Import packages
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fc1e8343100>




    
![output_2_1.png](/images/output_2_1.png)
    


*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fc1ca6ccfd0>




    
![output_4_1.png](/images/output_4_1.png)
    


### Example required *Spectural Clustering*

The clustering above is great! Then let's see some example which requires spectual clustering.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fc1d92eb8b0>




    
![output_6_1.png](/images/output_6_1.png)
    


For this plot, we can still make out two meaningful clusters. However, k-means won't work so well, because k-means is, by design, looking for circular clusters, not crescents.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fc1a81c0700>




    
![output_8_1.png](/images/output_8_1.png)
    


Whoops! That's not a good clustering! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, we will derive and implement spectral clustering. 

## Part A

>In this part, I will contruct the *similarity matrix* $\mathbf{A}$, which is a $n * n$ matrix. With using `epsilon = 0.4`, the entries `A[i,j]` should be equal to `1` if `X[i]` is within distance `epsilon` of `X[j]`, and `0` otherwise.

Note that **the diagonal entries `A[i,i]` should all be equal to zero.**


```python
# Set epsilon  = 0.4
epsilon = 0.4

# From the sklearn package import metrics
from sklearn import metrics
# Calculate a distance matrix
distance = metrics.pairwise_distances(X)
# For all the distances within epsilon, A[i,j] = 1,
# otherwise A[i, j] = 0
A = (distance < epsilon) * 1

# Set the diagonal entries A[i, i] as 0.
np.fill_diagonal(A, 0)
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B

The matrix `A` now contains information about which points are within distance `epsilon` with other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Here are the mathematics computation:

Let $d_i = \sum_{j = 1}^n a_{ij}$ be the $i$th row-sum of $\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of $\mathbf{A}$) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

A pair of clusters $C_0$ and $C_1$ is considered to be a "good" partition of the data when $N_{\mathbf{A}}(C_0, C_1)$ is small. To see why, let's look at each of the two factors in this objective function separately. 


### The cut term `cut(A, y)`

>In this part, I'll write a function `cut(A, y)` to compute the cut term $\mathbf{cut}(C_0, C_1)$, which is the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$.


```python
def cut(A, y):
    """
    This function compute the cut term by summing up the 
    entries A[i, j] for each pair of points (i, j)
    in different clusters.
    """

    # Create an integer 
    cut = 0
    
    for i in range(len(y)):
        for j in range(len(y)):
            # Here we only sum up the entries for each pair
            # of points in one cluster
            if y[i] == 0 and y[j] == 1:
                # Sum up all the different entries
                cut += A[i, j]
                
    return cut
```

Compute the cut objective for the true clusters `y`. Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels. 


```python
# Calculate the random objective for the random labels
cut_random = np.random.randint(0, 2, size = n)
cut(A, y), cut(A, cut_random)
```




    (13, 1150)



**In comparison, the cut objective for the true labels is *much* smaller than the cut objective for the random labels. ! This shows that this part of the cut objective indeed favors the true clusters over the random ones.**

### The Volume Term 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $C_0$ is a measure of how "big" cluster $C_0$ is. If we choose cluster $C_0$ to be small, then $\mathbf{vol}(C_0)$ will be small and $\frac{1}{\mathbf{vol}(C_0)}$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $C_0$ and $C_1$ such that:

1. There are relatively few entries of $\mathbf{A}$ that join $C_0$ and $C_1$. 
2. Neither $C_0$ and $C_1$ are too small. 

>Here, I write a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, returning them as a tuple `v0, v1`.


```python
def vols(A, y):
    """
    This function compute the volumes of C0 (y = 0) and C1 (y = 1),
    returning them as a tuple.
    """

    # Sum up the rows for each cluster
    v0, v1 = np.sum(A[y == 0]), np.sum(A[y == 1])

    return v0, v1
```

>Then, write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 


```python
def normcut(A, y):
    """
    This function use `cut(A, y)` and `vols(A, y)`
    to compute the binary nomralized cut objective of 
    A with y.
    """
    
    # Calculate by the given formula
    v0, v1 = vols(A, y)
    normcut = cut(A, y) * (1 / v0 + 1 / v1)

    return normcut
```

Now, compare the `normcut` objective using both the true labels `y` and the fake labels we just generated above. 


```python
# Compare the normcut for true labels
# and the fake labels
normcut(A, y), normcut(A, cut_random)
```




    (0.011518412331615225, 1.0240023597759158)



We can observe that the normcut for the true labels when compared to the normcut for the fake labels are much smaller.

## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $\mathbf{z}$ contain all the information from $\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$. 

Next, by linear algebra, we have 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  

###  `transform(A, y)`

>In this part, I write a function called `transform(A,y)` to compute the appropriate $\mathbf{z}$ vector given `A` and `y`, using the formula above. 


```python
def transform(A, y):
    """
    This function compute the appropriate `z`
    for y_i = 0 and y_i = 1
    """
    # Calculate the z vector
    v0, v1 = vols(A, y)
    z = np.where(y == 0, 1 / v0, -1 / v1)
    return z
```

>Then I check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 


```python
# Calculate z vector
z = transform(A, y)

# Calculate D as the diagonal matrix with
# suming the rows in A
D = np.zeros((n, n))
np.fill_diagonal(D, np.sum(A, axis = 1))

# Calculate the N_partc with given formula
N_partc = (z@(D - A)@z) / (z@D@z)

# Check of the `normcut()` and N_partc are close
np.isclose(normcut(A, y), N_partc)
```




    True



Awesome! The computation result of *norm cut objective* from **Part B** and **Part C** are close to each other!

>After that, also check the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $\mathbf{z}$ should contain roughly as many positive as negative entries. 


```python
# Check if the identity zT * D * 1 = 0
idtt = z.T@D@(np.ones(n))
np.isclose(idtt, 0)
```




    True



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. It's actually possible to bake this condition into the optimization, by substituting for $\mathbf{z}$ the orthogonal complement of $\mathbf{z}$ relative to $\mathbf{D}\mathbf{1}$. In the code below, I use the `orth_obj` function defines by Prof. Chodrow. 


```python
# These two function are defined by 
# Dr. Chorow
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Then I use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $\mathbf{z}$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_min`. 


```python
# Import the package
from scipy.optimize import minimize
z_min = minimize(orth_obj, z)
```

**Note**: there's a cheat going on here! We originally specified that the entries of $\mathbf{z}$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`, which is `z_min[i] = z_min.x`. 

Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 


```python
# z_min[i] = z_min.x
color = z_min.x < 0

plt.scatter(X[:,0], X[:,1], c = color)
```




    <matplotlib.collections.PathCollection at 0x7fc1e88766a0>




    
![output_36_1.png](/images/output_36_1.png)
    


Great job! It seems that we are close to correctly clustering!

## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $\mathbf{z}$, subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $\mathbb{1}$ is actually the eigenvector with smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 

> So, the vector $\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

### Calculate `z_eig`

In this part, I will

1. Construct *Laplacian* matrix of the similarity matrix $\mathbf{A}$ as $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$.

2. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`.

2. Then, plot the data again, using the sign of `z_eig` as the color.


```python
# Construct the matrix L
L = np.linalg.inv(D)@(D - A)

# Find the eigenvalues and eigenvectors
values, vectors = np.linalg.eig(L)
# Sort the eigenvalues
values_order = values.argsort()
values = values[values_order]
vectors = vectors[:, values_order]

# Then, find the second smallest eigenvector
z_eig = vectors[:, 1]
```


```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```




    <matplotlib.collections.PathCollection at 0x7fc1a824c8b0>




    
![output_40_1.png](/images/output_40_1.png)
    


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

>By the plot above, we can clearly see that the data are correctly clustering! Awesome!

## Part G

Synthesize the results from the previous parts. Now we can write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. Demonstrate a function using the supplied data from the beginning of the problem. 

###  `spectral_clustering(X, epsilon)`

I write this function by the following steps: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def spectral_clustering(X, epsilon):
    """
    This function return labels based on the eigenvector,
    by contructing the similarity matrix and the Laplacian matrix,
    computing the eigenvector with second-smallest eigenvalue.

    Argument
    --------
    X: The input array to be clustered
    epsilon: The distance threhold

    Return
    ------
    The labels based on the second-smallest eigenvalue
    """

    # First of all, I contruct the similarity matrix
    A = (metrics.pairwise_distances(X) < epsilon) * 1
    np.fill_diagonal(A, 0)

    # Secondly, I contructing the Laplacian matrix
    # Calculate D as the diagonal matrix with suming the rows in A
    D = np.zeros((n, n))
    np.fill_diagonal(D, np.sum(A, axis = 1))
    # And then we construct matrix
    # L = D^-1 (D-A)
    L = np.linalg.inv(D)@(D - A)

    # Thirdly, I compute the eigenvector with
    # second-smallest eigenvalue
    values, vectors = np.linalg.eig(L)
    # Sort the eigenvalues
    vectors = vectors[:,values.argsort()]
    # Find the second smallest eigenvector
    labels = (vectors[:, 1] > 0) * 1

    return labels
```


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```




    <matplotlib.collections.PathCollection at 0x7fc1a8409d90>




    
![output_44_1.png](/images/output_44_1.png)
    


>Compare this plot with the plots above, we can say that the function `spectral_clustering(X, epsilon)` successfully construct the spectral clustering for the data.

## Part H

Now, in this part, I run a few experiments using the function `spectral_clustering(X, epsilon)`, by generating different data sets using `make_moons` with different `noise = 0.1, 0.2` and `n = 500, 750, 1000`. 

### $n = 500$, $noise = 0.1$


```python
n = 500
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1e83d54f0>




    
![output_47_1.png](/images/output_47_1.png)
    


### $n = 500$, $noise = 0.2$


```python
n = 500
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1a970edc0>




    
![output_49_1.png](/images/output_49_1.png)
    


### $n = 750$, $noise = 0.1$


```python
n = 750
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1ca71cc10>




    
![output_51_1.png](/images/output_51_1.png)
    


### $n = 750$, $noise = 0.2$


```python
n = 750
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1d9600a00>




    
![output_53_1.png](/images/output_53_1.png)
    


### $n = 1000$, $noise = 0.1$


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1db076250>




    
![output_55_1.png](/images/output_55_1.png)
    


### $n = 1000$, $noise = 0.2$


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1a822dfa0>




    
![output_57_1.png](/images/output_57_1.png)
    


>In the comparison of all the plots above, we can conclude that when we increase `noise`, the distribution of points is more spread out. When we increase `n`, the distribution of points is still crescent-shaped, but more density.

## Part I

Now let's try the spectral clustering function on the bull's eye dataset! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fc1a970e910>




    
![output_60_1.png](/images/output_60_1.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fc1e91e4f70>




    
![output_62_1.png](/images/output_62_1.png)
    


Can the function successfully separate the two circles? To find this, we need some experimentation here with the value of `epsilon`. Here I 'll try values of `epsilon` between `0` and `1.0`.

### $epsilon = 0.11$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.11))
```




    <matplotlib.collections.PathCollection at 0x7fc1e923da60>




    
![output_64_1.png](/images/output_64_1.png)
    


### $epsilon = 0.12$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.12))
```




    <matplotlib.collections.PathCollection at 0x7fc1e92917c0>




    
![output_66_1.png](/images/output_66_1.png)
    


### $epsilon = 0.2$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```




    <matplotlib.collections.PathCollection at 0x7fc1ca740520>




    
![output_68_1.png](/images/output_68_1.png)
    


### $epsilon = 0.4$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```




    <matplotlib.collections.PathCollection at 0x7fc1e9a672b0>




    
![output_70_1.png](/images/output_70_1.png)
    


### $epsilon = 0.5$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```




    <matplotlib.collections.PathCollection at 0x7fc1aa909070>




    
![output_72_1.png](/images/output_72_1.png)
    


### $epsilon = 0.55$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.55))
```




    <matplotlib.collections.PathCollection at 0x7fc1aa9ced30>




    
![output_74_1.png](/images/output_74_1.png)
    


### $epsilon = 0.6$


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.6))
```




    <matplotlib.collections.PathCollection at 0x7fc1db11aac0>




![output_76_1.png](/images/output_76_1.png)
    


>In the comparison of all the plots above, we can see that when `epsilon` is smaller than $0.12$, the two circles are the same color, and when `epsilon` is larger than $0.5$, the two circles are not clustering in a right way.
Thus, **when `epsilon` is between $(0.12, 0.5)$, the function successfully separate two circles!**
