---
layout: post
title: Blog Post 5
---

In this post, I'll teach a machine learning algorithm to distinguish between pictures of dogs and pictures of cats. I'll use new skills and concepts related to image classfication in **Tensorflow**. 

>Notation: All the models in this blog post must include
1. at least two `Conv2D` layers
2. at least two `MaxPooling2D` layers
3. at least one `Flatten` layer
4. at least one `Dense` layer
5. at least one `Dropout` layer


# 1. Load Packages and Obtain Data

### Load Datasets

First of all, make a code block in which I'll hold my `import` statements. This code block will be updated in the following steps.

```python
import os
from tensorflow.keras import utils
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models, losses
```

Then, access the data. I'll use a sample dataset from the TensorFlow team that contains labeles images of cats and dogs.

In this case, I'll use a special-purpose `keras` utility called `image_dataset_from_directory` to construct a dataset. The argumnets are:

1. The first and the most important argument is the first one, which says **where** the images are located.
2. `shuffle`, says that when retrieving data from this directory, the order should be randomized.
3. `batch_size`, determines how many data points are gathered from the directory at once.
4. `image_size`, specifies the size of the input images.

The code shown below are provided by Prof. Chodrow. 

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```




    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.

Awesome! Now we have created TensorFlow datasets for training, validation and testing.

Then we need to read data rapidly. The code is shown below, provided by Prof. Chodrow.

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

### Working with Datasets

Now let's briefly explore our dataset. I'll write a function to create a two-row visualization:

1. In the first row, show three random pictures of cats.
2. In the second row, show three random pictures of dogs.

```python
def two_row_visualization():
  # Decide the size of the pictures
  plt.figure(figsize = (12, 12))
  for images, labels in train_dataset.take(1):
    for i in range(3):
      # Display the cat pics in the first row
      ax = plt.subplot(3, 3, i+1)
      plt.imshow(images[labels == 0][i].numpy().astype("uint8"))
      plt.title('Cat')
      plt.axis("off")
      # Display the dog pics in the second row
      ax = plt.subplot(3, 3, i+4)
      plt.imshow(images[labels == 1][i].numpy().astype("uint8"))
      plt.title('Dog')
      plt.axis("off")

two_row_visualization()
```
![5-1.png](/images/5-1.png)

The pictures looks great! 

### Check Label Frequencies

In the next step, I'll use a line of code provided by Prof. Chodrow to create an iterator called `labels`.

```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

Compute the number of images in the training data with label `0` (corresponding to `"cat"`) and label `1` (corresponding to `"dog"`).

Then let's check the frequencies.

```python
# The frequecy for cat
cat_fre = sum(train_dataset.unbatch().map(lambda image, label: label == 0).as_numpy_iterator())

# The frequecy for dog
dog_fre = sum(train_dataset.unbatch().map(lambda image, label: label == 1).as_numpy_iterator())

# The baseline accuracy of the model
cat_fre, dog_fre, cat_fre / (cat_fre + dog_fre), dog_fre / (cat_fre + dog_fre)
```




    (1000, 1000, 0.5, 0.5)

Thus, we know that there are 1,000 cat pictures and 1,000 dog pictures. **The baseline accuracy of our model is 50%.**

# 2. First Model

In the following part, I'll create a `tf.keras.Sequential` model using some of the layers.

These layers shoule be imported at the first chunk of the blog.

**Note: The `model1` should achieve at least 52% validation accuracy in this part.**

```python
# Build our first model

model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)), 
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.4),
    # We have 2 classes for cat and dog
    layers.Dense(2)
])
```

Our model is all set, let's train our model!

The training requires two steps:
1.   Compile the model
2.   Fit the model for training

```python
# Compile the model
model1.compile(optimizer='adam',
               loss=losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

# Fit the model
history1 = model1.fit(train_dataset,
                      epochs = 20,
                      validation_data = validation_dataset)
```




    Epoch 1/20
    63/63 [==============================] - 7s 89ms/step - loss: 6.3901 - accuracy: 0.5455 - val_loss: 0.6793 - val_accuracy: 0.5978
    Epoch 2/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6704 - accuracy: 0.5850 - val_loss: 0.6558 - val_accuracy: 0.6250
    Epoch 3/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6000 - accuracy: 0.6805 - val_loss: 0.6761 - val_accuracy: 0.5767
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5702 - accuracy: 0.7175 - val_loss: 0.7143 - val_accuracy: 0.6040
    Epoch 5/20
    63/63 [==============================] - 8s 120ms/step - loss: 0.5052 - accuracy: 0.7495 - val_loss: 0.9013 - val_accuracy: 0.5928
    Epoch 6/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4331 - accuracy: 0.7985 - val_loss: 0.8564 - val_accuracy: 0.6089
    Epoch 7/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.3574 - accuracy: 0.8410 - val_loss: 0.8666 - val_accuracy: 0.6176
    Epoch 8/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.3024 - accuracy: 0.8740 - val_loss: 0.9967 - val_accuracy: 0.5656
    Epoch 9/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.2769 - accuracy: 0.8810 - val_loss: 1.0338 - val_accuracy: 0.6275
    Epoch 10/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.1881 - accuracy: 0.9315 - val_loss: 1.2970 - val_accuracy: 0.6101
    Epoch 11/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.1901 - accuracy: 0.9240 - val_loss: 1.2189 - val_accuracy: 0.5965
    Epoch 12/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1523 - accuracy: 0.9455 - val_loss: 1.2912 - val_accuracy: 0.5718
    Epoch 13/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.1272 - accuracy: 0.9615 - val_loss: 1.4775 - val_accuracy: 0.5978
    Epoch 14/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.1121 - accuracy: 0.9635 - val_loss: 1.4929 - val_accuracy: 0.5978
    Epoch 15/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.1015 - accuracy: 0.9625 - val_loss: 1.6758 - val_accuracy: 0.6052
    Epoch 16/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.0808 - accuracy: 0.9690 - val_loss: 1.7480 - val_accuracy: 0.6052
    Epoch 17/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.0978 - accuracy: 0.9665 - val_loss: 1.5536 - val_accuracy: 0.5780
    Epoch 18/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0710 - accuracy: 0.9745 - val_loss: 1.7538 - val_accuracy: 0.5965
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.0673 - accuracy: 0.9770 - val_loss: 1.9947 - val_accuracy: 0.5656
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.0649 - accuracy: 0.9740 - val_loss: 1.7405 - val_accuracy: 0.5829

Looks good! Now we can graph the accuracy of the training and validation, compared `model1` with the validation accuracy 52%.

```python
# Plot the training accuracy
plt.plot(history1.history['accuracy'], label = "Training Set Accuracy")

# Plot the validation accuracy
plt.plot(history1.history['val_accuracy'], label = "Validation Accuracy")

# Add the lower bound accuracy
plt.ylim([0.45, 1])
plt.axhline(y = 0.52, color = "black",
            label = "Lowest accuracy = 52%")
plt.xlabel("epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy of Training Set and Validation")
plt.legend()
plt.show()
```
![5-2.png](/images/5-2.png)

By the plot above, we can see that

1. **The accuracy of my first model stabilized between 56% andn 62% during training**.
2. Compared to the baseline as 50%, the highest validation accuracy is 62%, which improve 12%.
3. The accuracy of training set is much more higher than the validation, which indicates this model is overfitting.

# 3. Model with Data Augumentation

Now, I'll add some **data augumentation** layers to the model. 

The **data augumentation** refers to the practice of including modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process in order to help our model learn so-called *invariant* features of our input images.

**`tf.keras.layers.RandomFlip()`**

First, I'll create a `tf.keras.layers.RandomFlip()` layer and make a plot of the original image and a few copies to which `RandomFlip()` has been applied.

```python
flipper = tf.keras.Sequential([
    layers.RandomFlip()
])

for image, _ in train_dataset.take(1):
    plt.figure(figsize=(12, 12))
    first_image = image[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i+1)
        augmented_image = flipper(tf.expand_dims(first_image, 0))
        plt.imshow(augmented_image[0] / 255)
        plt.axis('off')
```
![5-3.png](/images/5-3.png)

The `flipper` did a good job!

**`tf.keras.layers.RandomRotation()`**

Next, I'll create a `tf.keras.layers.RandomRotation()` layer. check the docs to learn more about the arguments accepted by this layer. And then, make a plot of both the original image and a few copies to which `RandomRotation()` has been applied.

```python
rotater = tf.keras.Sequential([
    layers.RandomRotation(0.2)    
])

for image, _ in train_dataset.take(1):
    plt.figure(figsize=(12, 12))
    first_image = image[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i+1)
        augmented_image = rotater(tf.expand_dims(first_image, 0))
        plt.imshow(augmented_image[0] / 255)
        plt.axis('off')
```
![5-4.png](/images/5-4.png)

Great! The `rotater` works well!

## Model 2

Now, create a new `tf.keras.models.Sequential` as `model2` in which the first two layers are augumentation layers. I'll use the `RandomFlip()` layer and a `RandomRotation()` layer. 

**Note: The `model2` should achieve at least 55% validation accuracy in this part.**

```python
# Build our second model

model2 = tf.keras.Sequential([
    # Data Augumentation
    layers.RandomFlip(),
    layers.RandomRotation(0.2),

    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)), 
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.4),
    # We have 2 classes for cat and dog
    layers.Dense(2)
])
```
Now let's train our `model2` and visualize the training history like what we did for `model1`.

```python
# Compile the model
model2.compile(optimizer='adam',
               loss=losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

# Fit the model
history2 = model2.fit(train_dataset,
                      epochs = 20,
                      validation_data = validation_dataset)
```




    Epoch 1/20
    63/63 [==============================] - 7s 83ms/step - loss: 3.1334 - accuracy: 0.5270 - val_loss: 0.6855 - val_accuracy: 0.5606
    Epoch 2/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6892 - accuracy: 0.5500 - val_loss: 0.6791 - val_accuracy: 0.5804
    Epoch 3/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6846 - accuracy: 0.5700 - val_loss: 0.6801 - val_accuracy: 0.5916
    Epoch 4/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.6798 - accuracy: 0.5930 - val_loss: 0.6666 - val_accuracy: 0.5953
    Epoch 5/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6798 - accuracy: 0.5870 - val_loss: 0.6853 - val_accuracy: 0.5631
    Epoch 6/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6748 - accuracy: 0.5935 - val_loss: 0.6819 - val_accuracy: 0.5705
    Epoch 7/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6825 - accuracy: 0.5780 - val_loss: 0.6669 - val_accuracy: 0.5928
    Epoch 8/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6697 - accuracy: 0.5965 - val_loss: 0.6556 - val_accuracy: 0.6126
    Epoch 9/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6615 - accuracy: 0.6125 - val_loss: 0.6432 - val_accuracy: 0.6312
    Epoch 10/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6535 - accuracy: 0.6225 - val_loss: 0.6520 - val_accuracy: 0.6250
    Epoch 11/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6473 - accuracy: 0.6365 - val_loss: 0.6625 - val_accuracy: 0.5965
    Epoch 12/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6616 - accuracy: 0.5970 - val_loss: 0.6526 - val_accuracy: 0.6213
    Epoch 13/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6563 - accuracy: 0.6060 - val_loss: 0.6594 - val_accuracy: 0.5903
    Epoch 14/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6522 - accuracy: 0.6300 - val_loss: 0.6391 - val_accuracy: 0.6176
    Epoch 15/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6363 - accuracy: 0.6445 - val_loss: 0.6363 - val_accuracy: 0.6287
    Epoch 16/20
    63/63 [==============================] - 7s 107ms/step - loss: 0.6393 - accuracy: 0.6320 - val_loss: 0.6319 - val_accuracy: 0.6312
    Epoch 17/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6278 - accuracy: 0.6600 - val_loss: 0.6394 - val_accuracy: 0.6262
    Epoch 18/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6410 - accuracy: 0.6285 - val_loss: 0.6712 - val_accuracy: 0.5928
    Epoch 19/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6762 - accuracy: 0.5555 - val_loss: 0.6371 - val_accuracy: 0.6101
    Epoch 20/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6224 - accuracy: 0.6570 - val_loss: 0.6606 - val_accuracy: 0.6064

Now we can graph the accuracy of the training and validation, compared `model2` with the validation accuracy 55%.

```python
# Plot the training accuracy
plt.plot(history2.history['accuracy'], label = "Training Set Accuracy")

# Plot the validation accuracy
plt.plot(history2.history['val_accuracy'], label = "Validation Accuracy")

# Add the lower bound accuracy
plt.ylim([0.45, 1])
plt.axhline(y = 0.55, color = "black",
            label = "Lowest accuracy = 55%")
plt.xlabel("epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy of Training Set and Validation")
plt.legend()
plt.show()
```
![5-5.png](/images/5-5.png)

By the plot above, we can see that

1. **The accuracy of my first model stabilized between 56% andn 63% during training**.
2. Compared to the highest accuracy of `model1` as 62%, the highest validation accuracy is 63%, which indicates `model2` accuracy is slightly higher than `model1`.
3. The accuracy of training set is quite close to the validation. No overfitting in the `model2` can be observed.

# 4. Data Preprocessing

Sometimes, we can make simple transformations to the input data to help data preprocessing.

For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. These are mathematically identical situations, since we can always just scale the weights. But if we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.

The following code, provide by Prof. Chodrow, will create a preprocessing layer called `preprocessor` which I can slot into the model pipeline.

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```
Then, I use the `preprocessor` layer as the first layer in building `model3`.

```python
# Build our third model

model3 = tf.keras.Sequential([
    # Data Preprocessing
    preprocessor,

    # Data Augumentation
    layers.RandomFlip(),
    layers.RandomRotation(0.2),

    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)), 
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.1),
    # We have 2 classes for cat and dog
    layers.Dense(2)
])
```

Now let's train our `model3`.

```python
# Compile the model
model3.compile(optimizer='adam',
               loss=losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

# Fit the model
history3 = model3.fit(train_dataset,
                      epochs = 20,
                      validation_data = validation_dataset)
```




    Epoch 1/20
    63/63 [==============================] - 9s 89ms/step - loss: 0.6930 - accuracy: 0.5410 - val_loss: 0.6699 - val_accuracy: 0.6188
    Epoch 2/20
    63/63 [==============================] - 10s 151ms/step - loss: 0.6766 - accuracy: 0.5710 - val_loss: 0.6517 - val_accuracy: 0.5792
    Epoch 3/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.6534 - accuracy: 0.5880 - val_loss: 0.6199 - val_accuracy: 0.6349
    Epoch 4/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6377 - accuracy: 0.6255 - val_loss: 0.6169 - val_accuracy: 0.6510
    Epoch 5/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6186 - accuracy: 0.6545 - val_loss: 0.6384 - val_accuracy: 0.6337
    Epoch 6/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6071 - accuracy: 0.6680 - val_loss: 0.6268 - val_accuracy: 0.6361
    Epoch 7/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5922 - accuracy: 0.6795 - val_loss: 0.5749 - val_accuracy: 0.6931
    Epoch 8/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5835 - accuracy: 0.6930 - val_loss: 0.5933 - val_accuracy: 0.6782
    Epoch 9/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5727 - accuracy: 0.7025 - val_loss: 0.5702 - val_accuracy: 0.7054
    Epoch 10/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5652 - accuracy: 0.7035 - val_loss: 0.5613 - val_accuracy: 0.6980
    Epoch 11/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5645 - accuracy: 0.7225 - val_loss: 0.5704 - val_accuracy: 0.6881
    Epoch 12/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5563 - accuracy: 0.7055 - val_loss: 0.5691 - val_accuracy: 0.7017
    Epoch 13/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5497 - accuracy: 0.7230 - val_loss: 0.5676 - val_accuracy: 0.7129
    Epoch 14/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5245 - accuracy: 0.7470 - val_loss: 0.5648 - val_accuracy: 0.7104
    Epoch 15/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.5348 - accuracy: 0.7275 - val_loss: 0.5720 - val_accuracy: 0.7017
    Epoch 16/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.5198 - accuracy: 0.7445 - val_loss: 0.5765 - val_accuracy: 0.7153
    Epoch 17/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.5161 - accuracy: 0.7455 - val_loss: 0.5071 - val_accuracy: 0.7488
    Epoch 18/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5033 - accuracy: 0.7390 - val_loss: 0.5289 - val_accuracy: 0.7376
    Epoch 19/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5044 - accuracy: 0.7530 - val_loss: 0.5040 - val_accuracy: 0.7512
    Epoch 20/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.4979 - accuracy: 0.7520 - val_loss: 0.5177 - val_accuracy: 0.7488

Now we can graph the accuracy of the training and validation, compared `model3` with the validation accuracy 70%.

```python
# Plot the training accuracy
plt.plot(history3.history['accuracy'], label = "Training Set Accuracy")

# Plot the validation accuracy
plt.plot(history3.history['val_accuracy'], label = "Validation Accuracy")

# Add the lower bound accuracy
plt.ylim([0.45, 1])
plt.axhline(y = 0.70, color = "black",
            label = "Lowest accuracy = 70%")
plt.xlabel("epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy of Training Set and Validation")
plt.legend()
plt.show()
```
![5-6.png](/images/5-6.png)

By the plot above, we can see that

1. **The accuracy of my first model stabilized between 57% andn 75% during training**.
2. Compared to the highest accuracy of `model2` as 63%, the highest validation accuracy is 75%, which indicates `model3` accuracy is much higher than `model1` and `model2`.
3. The accuracy of training set is quite close to the validation. No overfitting in the `model3` can be observed.

# 5. Transfer Learning

So far, I’ve been training models for distinguishing between cats and dogs from scratch. However, sometimes people might already have trained a model that does a related task, and might have learned some relevant patterns. 

Thus, in the this part, I'd like to use a pre-existing model for our task.

To do this, I first access a pre-existing "base model", incorporate it into a full model for our current task, and then train that model by the following code provided by Prof. Chodrow.

```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, I'll create a model called `model4` using `MobileNetV2`. These layers will be used:
1. `preprocessor` from part 4
2. data augmentation layers from part 3.
3. `base_model_layer` we just constructed above
4. A `dense(2)` layer at the end to actually perform the classification.

Let's begin!

```python
# Build our fourth model

model4 = tf.keras.Sequential([
    # Data Preprocessing
    preprocessor,

    # Data Augumentation
    layers.RandomFlip(),
    layers.RandomRotation(0.2),

    # Train the base model
    base_model_layer,

    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.1),
    layers.Dense(2)
])
```

Then train `model4` and visualize the training history.

```python
# Compile the model
model4.compile(optimizer='adam',
               loss=losses.SparseCategoricalCrossentropy(from_logits=True),
               metrics=['accuracy'])

# Fit the model
history4 = model4.fit(train_dataset,
                      epochs = 20,
                      validation_data = validation_dataset)
```




    Epoch 1/20
    63/63 [==============================] - 14s 116ms/step - loss: 0.3799 - accuracy: 0.8260 - val_loss: 0.1095 - val_accuracy: 0.9579
    Epoch 2/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.2253 - accuracy: 0.9055 - val_loss: 0.0864 - val_accuracy: 0.9678
    Epoch 3/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.2018 - accuracy: 0.9175 - val_loss: 0.0793 - val_accuracy: 0.9752
    Epoch 4/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.2019 - accuracy: 0.9170 - val_loss: 0.0751 - val_accuracy: 0.9715
    Epoch 5/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1813 - accuracy: 0.9250 - val_loss: 0.0742 - val_accuracy: 0.9752
    Epoch 6/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.1657 - accuracy: 0.9275 - val_loss: 0.0642 - val_accuracy: 0.9715
    Epoch 7/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.1829 - accuracy: 0.9210 - val_loss: 0.0706 - val_accuracy: 0.9790
    Epoch 8/20
    63/63 [==============================] - 7s 112ms/step - loss: 0.1775 - accuracy: 0.9305 - val_loss: 0.0827 - val_accuracy: 0.9728
    Epoch 9/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1769 - accuracy: 0.9210 - val_loss: 0.0657 - val_accuracy: 0.9777
    Epoch 10/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.1674 - accuracy: 0.9290 - val_loss: 0.0631 - val_accuracy: 0.9765
    Epoch 11/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.1594 - accuracy: 0.9315 - val_loss: 0.0638 - val_accuracy: 0.9752
    Epoch 12/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1502 - accuracy: 0.9410 - val_loss: 0.0607 - val_accuracy: 0.9802
    Epoch 13/20
    63/63 [==============================] - 7s 113ms/step - loss: 0.1436 - accuracy: 0.9435 - val_loss: 0.0611 - val_accuracy: 0.9765
    Epoch 14/20
    63/63 [==============================] - 7s 108ms/step - loss: 0.1619 - accuracy: 0.9365 - val_loss: 0.0612 - val_accuracy: 0.9827
    Epoch 15/20
    63/63 [==============================] - 9s 128ms/step - loss: 0.1425 - accuracy: 0.9410 - val_loss: 0.0567 - val_accuracy: 0.9802
    Epoch 16/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1482 - accuracy: 0.9365 - val_loss: 0.0654 - val_accuracy: 0.9790
    Epoch 17/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1458 - accuracy: 0.9390 - val_loss: 0.0788 - val_accuracy: 0.9765
    Epoch 18/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.1358 - accuracy: 0.9495 - val_loss: 0.0762 - val_accuracy: 0.9752
    Epoch 19/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1380 - accuracy: 0.9415 - val_loss: 0.0573 - val_accuracy: 0.9802
    Epoch 20/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.1326 - accuracy: 0.9470 - val_loss: 0.0579 - val_accuracy: 0.9765

Graph the accuracy of the training and validation, compared `model4` with the validation accuracy 95%.

```python
# Plot the training accuracy
plt.plot(history4.history['accuracy'], label = "Training Set Accuracy")

# Plot the validation accuracy
plt.plot(history4.history['val_accuracy'], label = "Validation Accuracy")

# Add the lower bound accuracy
plt.ylim([0.45, 1])
plt.axhline(y = 0.95, color = "black",
            label = "Lowest accuracy = 95%")
plt.xlabel("epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy of Training Set and Validation")
plt.legend()
plt.show()
```
![5-7.png](/images/5-7.png)

By the plot above, we can see that

1. **The accuracy of my first model stabilized between 96% andn 98% during training**.
2. Compared to the highest accuracy of `model3` as 75%, the highest validation accuracy is 98%, which indicates `model4` accuracy is much higher than `model1`, `model2` and `model3`.
3. No overfitting in the `model4` can be observed.

# 6. Score on Test Data

Finally, let's evaluate `model4` on the unseen `test_dataset`.

```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy of my most performant model on unseen test set is ',accuracy)
```




    6/6 [==============================] - 1s 62ms/step - loss: 0.0402 - accuracy: 0.9896
    Test accuracy of my most performant model on unseen test set is  0.9895833134651184

Well done! The final model achieve a high accuracy at 98.95%, which indicates our model is successful!

**Thank you for your reading!**